# -*- coding: utf-8 -*-
"""NPL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14dasA5K7ZE68FngWFbmNyZbTz0dbM4gx
"""

import nltk
#descargamos el corpus de texto que queramos o vamos a trabajar
nltk.download("cess_esp")

import re
corpus = nltk.corpus.cess_esp.sents()
print(corpus)
#len de titulos 6030
print(len(corpus))

lista = [word for i in corpus for word in i]
print(lista[:20])

# usarmeos el metodo search() de "re" para buscar cietros patrones en nuestra lista corpus
array = [word for word in lista if re.search("[a]{2}", word)] #patron que encuentra mas de 2 "a" en una palabra
print(array)

array = [word for word in lista if re.search("es$", word)] # patron que encuentra las palabras que terminan en "es"
print(array)


array = [word for word in lista if re.search("^es", word)] # patron que encuentra las palabras que comienzan en "es"
print(array)

#usamos un rango de expresion regular
array = [word for word in lista if re.search("^[es]", word)] # patron que encuentra las palabras que contenagn la los caracteres "e" o "s" o las 2
print(array)

"""# Clausuras"""

# las clausuras se clasifican en dos importantes grupos:
# * que la palabra se puede repetir 0 o mas veces
# + que la palabra se puede repetir 1 o mas veces
# ejemplo con *
array = [word for word in lista if re.search("^(h)*", word)]
print(array)
array = [word for word in lista if re.search("(h)+$", word)]
print(array)

"""# **tokenización con expresiones regulares**"""

#print("esta es \n una prueba")
#print(r"esta es \n una prueba")

texto = """ 
          Cuando sea el rey del mundo  (imaginaba él en su cabeza) no tendré que  preocuparme por estas bobadas. 
          Era solo un niño de 7 años, pero pensaba que podría ser cualquier cosa que su imaginación le permitiera visualizar en su cabeza ...
            
        """

# 1er caso tokenimos por espacios en blanco
print(re.split(r" ", texto))

#2do caso tokenizamos usanod regex
#elimino espacios tabs o saltos de linea de mi texto
print(re.split(r"[ \t\n]+", texto))
# caso 3 dejamos solo las letras quitamos parentesis con \W regex
print(re.split(r"[ \W\t\n]+", texto))

"""# **TOKENIZADOR NLTK**
-------------------------------------------
"""

texto = "En los E.U.  esa postal vale $15.50 ..."
print(re.split("[ \t\n]+", texto)) # esta es la forma que entiendo y es chirri

#forma profesional de hacerlo :( y que maso entiendo
pattern = r'''(?x)                  # Flag para iniciar el modo verbose
              (?:[A-Z]\.)+            # Hace match con abreviaciones como U.S.A.
              | \w+(?:-\w+)*         # Hace match con palabras que pueden tener un guión interno
              | \$?\d+(?:\.\d+)?%?  # Hace match con dinero o porcentajes como $15.5 o 100%
              | \.\.\.              # Hace match con puntos suspensivos
              | [][.,;"'?():-_`]    # Hace match con signos de puntuación
            '''
print(nltk.regexp_tokenize(texto, pattern))

"""# **Estadisticas Basicas del lenguaje**
---------------------------------------------------------------------
"""

# descargamos el dataset "book"
import nltk
nltk.download("book")
# importamos todo el dataset
from nltk.book import *
import matplotlib.pyplot as plt
import numpy as np

#a = [text for text in dir(nltk.book) if re.search(r"text\d", text)]
text1.tokens[:10]
# numero de tokens del corpus
print(len(text1))

"""# ***MEDIDA DE RIQUEZA DE PALABRAS //***
---------------------------------------------------------------------
R = total de palabras unicas
    ------------------------
    R = total de palabras unicas        longitud vocabulario  
        ------------------------   =   -----------------
        total de palabras               longitud texto
"""

vocabulario = sorted(set(text1)) # sacamos las palabras unicas
print(vocabulario)

# calculamos la riqueza lexica de text1
r = len(vocabulario) / len(text1)

def riqueza_lexica(texto):
  vocabulario = sorted(set(texto)) # sacamos las palabras unicas  
  return len(vocabulario) / len(texto)

#print(riqueza_lexica(text2))
# AHORA HAREMOS UNA FUNCION QUE NOS BOTARÁ EL PORCENTAJE DE CUANTAS VECES UNA PALABRA ESTA DENTRO DE UN TEXTO :)
def porcentaje_palabra(palabra, texto):
  return(100*texto.count(palabra) / len(texto))

#porcentaje_palabra("palas", text1)

"""# **DISTRIBUCION DE FRECUENCIA DE PALABRAS**
---------------------------------------------------------------------

vamos a crear un dict que contega  key=word: value=text.count(word)
"""

# Commented out IPython magic to ensure Python compatibility.
# # manera menos eficiente de mostrar las distribuciones de palabras de un texto
# %%timeit
# dict = {}
# for word in set(text1):
#   dict[word] = text1.count(word)
# dict

#la manera mas eficinete y rapida es usando un metodo de nltk FreqDist
#%%timeit
result = FreqDist(text1)
result.most_common(20) # aqui le digo que me muestre los 20 tokens  mas comunes
type(result.most_common(20))
lista_words = []
lista_count = []
for i in result.most_common(20):
    lista_words.append(i[0])
    lista_count.append(i[1])
    #print(i[0], i[1])
print(lista_words)
print(lista_count)
fig, ax = plt.subplots()
ax.plot(lista_words, lista_count)

ax.set(xlabel='palabras más comunes', ylabel='numero de veces',
       title='Distribucion de palabras')

plt.show()

# NOTA: no hay necesidad de usar matplotlib directamente y procesar la data a mostrar sino puedes usar fdist de nltk (+isy) ahi si allá sumerce



"""# **Refinamiento y visualización de cuerpos de texto**

* Como vimos en la sección anterior, los tokens más frecuentes en un texto no son necesariamente las palabras que mas informacion nos arrojan sobre el contenido del mismo. 
* Por ello, es mejor filtrar y construir distribuciones de frecuencia que no consideren signos de puntuación o caracteres especiales


---
"""

lista_pal_imp = [palabra for palabra in text1 if len(palabra) > 5] # verificamos que la salida dara palabras distinstas signos de puntuación y esas mkds
lista_pal_imp
# ahora ordenamos la lista y pa fuera los repetidos
palabras_filters = sorted(set(lista_pal_imp))
palabras_filters
def porcentaje_palabra(palabra, texto):
  return(texto.count(palabra))
#print(porcentaje_palabra("the", text1))
# hacemos una lista de tuplas de las palabras y si fdist[palabra] osea el count de cuantas veces aparece esa palabra
palabras_interesantes = [(palabra, porcentaje_palabra(palabra, text1)) for palabra in set(text1) if len(palabra)>5 and porcentaje_palabra(palabra, text1)>20]
palabras_interesantes

# ahora psaremos a objetos numpy nuestra distribucion de palabras
dtypes = [("palabra", "S10"), ("frecuencia", int)]
palabras_interesantes = np.array(palabras_interesantes, dtype=dtypes)
palabras_interesantes # aqui ya tenemos nuestro objeto numpy
# ahora aqui ordeno mi numpy array
palabras_interesantes = np.sort(palabras_interesantes, order="frecuencia")
palabras_interesantes

# salu2 plots con matplotlib :3
top_words = 20 #vamos atomar las 20 palabras mas repetidas y pues las mas repetidas como ya ordenamos nuestro array numpy seran las 20 ultimas
# array = [1,2,3,4] array[-2:] 2 ultimos numeros del array
x = np.arange(len(palabras_interesantes[-top_words:]))
y = [freq[1] for freq in palabras_interesantes[-top_words:]]
plt.figure(figsize=(10, 5))
plt.plot(x, y)
plt.xticks(x, [str(freq[0]) for freq in palabras_interesantes[-top_words:]], rotation = 'vertical')
plt.ylabel("cantidad de veces que aparece tokens en el texto")
plt.grid(True)
plt.show()